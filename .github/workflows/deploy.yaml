name: Deploy to Databricks

on:
  push:
    branches:
      - main

jobs:
  CD:
    runs-on: ubuntu-latest
    outputs:
      databricks_host: ${{ steps.set-env.outputs.databricks_host }}
      databricks_token: ${{ steps.set-env.outputs.databricks_token }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install databricks CLI
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install azure-identity azure-keyvault-secrets requests

      - name: Retrieve secrets from Azure Key Vault
        id: set-env
        env:
          AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
          AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
          AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
          AZURE_KEY_VAULT_NAME: ${{ secrets.AZURE_KEY_VAULT_NAME }}
        run: |
          echo "Retrieving secrets from Azure Key Vault..."
          python -c "
          import os
          from azure.identity import DefaultAzureCredential
          from azure.keyvault.secrets import SecretClient

          key_vault_name = os.getenv('AZURE_KEY_VAULT_NAME')
          key_vault_uri = f'https://{key_vault_name}.vault.azure.net'

          credential = DefaultAzureCredential()
          client = SecretClient(vault_url=key_vault_uri, credential=credential)

          databricks_host = client.get_secret('cdv-big-data-adb-url').value
          databricks_token = client.get_secret('cdv-big-data-adb-token').value
          git_repo_url = client.get_secret('cdv-big-data-git-url').value
          git_branch = client.get_secret('cdv-big-data-git-branch').value

          print(f'set-output name=databricks_host::{databricks_host}')
          print(f'::set-output name=databricks_token::{databricks_token}')
          with open(os.getenv('GITHUB_ENV'), 'a') as env_file:
            env_file.write(f'DATABRICKS_HOST={databricks_host}\n')
            env_file.write(f'DATABRICKS_TOKEN={databricks_token}\n')
            env_file.write(f'GIT_REPO_URL={git_repo_url}\n')
            env_file.write(f'GIT_BRANCH={git_branch}\n')
          "
          
      - name: Clone Git repository
        run: |
          git clone ${{ env.GIT_REPO_URL }} --branch ${{ env.GIT_BRANCH }} --single-branch

      - name: Repo to Archive
        uses: actions/upload-artifact@v4
        id: save_repo
        with:
          name: big_data
          path: big_data/


      - name: Deploy to Databricks
        run: |
          databricks workspace import-dir big_data /cdv_big_data/big_data --overwrite

      - name: Deploy Workflow using REST API
        run: |
          python -c "
          import os
          import json
          import requests

          databricks_host = os.getenv('DATABRICKS_HOST')
          databricks_token = os.getenv('DATABRICKS_TOKEN')

          file_names = os.listdir('big_data/orchestration/')
          workflows_list = [file_name.split('.')[0] for file_name in file_names]
          print(workflows_list)
              
          for workflow in workflows_list:
              try:
                  with open(f'big_data/orchestration/{workflow}.json', 'r') as f:
                    orc_config = json.load(f)
                  print(orc_config)
                  
                  headers = {
                      'Authorization': f'Bearer {databricks_token}',
                      'Content-Type': 'application/json'
                  }
                  
                  try:
                      response = requests.get(f'{databricks_host}/api/2.2/jobs/list', headers=headers)
                      for job in response.json()['jobs']:
                          if job['settings']['name'] in workflows_list:
                              job_id = job['job_id']
                              response = requests.post(f'{databricks_host}/api/2.2/jobs/reset', headers=headers, 
                                                      json={'job_id': job_id,
                                                            'new_settings': orc_config
                                                            })
                              response.raise_for_status()
                              print(f'''Updated workflow_name: {job['settings']['name']} / job_id: {job_id})''')
                          else:
                              response = requests.post(f'{databricks_host}/api/2.2/jobs/create', headers=headers, json=orc_config)
                              response.raise_for_status()
                              print(f'''Added workflow_name: {job['settings']['name']} / job_id: {job_id}''')
                  except Exception as e:
                      print(e)
              except FileNotFoundError:
                  print('File not found')
          "