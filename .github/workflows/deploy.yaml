name: Deploy to Databricks

on:
  push:
    branches:
      - main
    # paths:
    #   - 'big_data/**'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      databricks_host: ${{ steps.set-env.outputs.databricks_host }}
      databricks_token: ${{ steps.set-env.outputs.databricks_token }}
    steps:
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'
    - name: Install databricks CLI
      run: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install azure-identity azure-keyvault-secrets requests

    - name: Retrieve secrets from Azure Key Vault
      env:
        AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
        AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
        AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
        AZURE_KEY_VAULT_NAME: ${{ secrets.AZURE_KEY_VAULT_NAME }}
      run: |
        echo "Retrieving secrets from Azure Key Vault..."
        python -c "
        import os
        from azure.identity import DefaultAzureCredential
        from azure.keyvault.secrets import SecretClient

        key_vault_name = os.getenv('AZURE_KEY_VAULT_NAME')
        key_vault_uri = f'https://{key_vault_name}.vault.azure.net'

        credential = DefaultAzureCredential()
        client = SecretClient(vault_url=key_vault_uri, credential=credential)

        databricks_host = client.get_secret('cdv-big-data-adb-url').value
        databricks_token = client.get_secret('cdv-big-data-adb-token').value
        git_repo_url = client.get_secret('cdv-big-data-git-url').value
        git_branch = client.get_secret('cdv-big-data-git-branch').value

        with open(os.getenv('GITHUB_ENV'), 'a') as env_file:
            env_file.write(f'DATABRICKS_HOST={databricks_host}\n')
            env_file.write(f'DATABRICKS_TOKEN={databricks_token}\n')
            env_file.write(f'GIT_REPO_URL={git_repo_url}\n')
            env_file.write(f'GIT_BRANCH={git_branch}\n')
        "
  CD:
    runs-on: ubuntu-latest
    needs: setup
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Configure Databricks CLI
      run: |
        echo -e "${{ needs.setup.outputs.databricks_host }}\n${{ needs.setup.outputs.databricks_token }}" | databricks configure --token

    - name: Clone Git repository
      run: |
        git clone ${{ env.GIT_REPO_URL }} --branch ${{ env.GIT_BRANCH }} --single-branch

    - name: Deploy to Databricks
      run: |
        databricks workspace import-dir big_data /cdv_big_data/big_data --overwrite
  Workflows:
    runs-on: ubuntu-latest
    needs: setup
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Configure Databricks CLI
      run: |
        echo -e "${{ needs.setup.outputs.databricks_host }}\n${{ needs.setup.outputs.databricks_token }}" | databricks configure --token

    - name: Deploy Workflow
      run: |
        cat <<EOF > workflow_config.json
        {
          "name": "ORC Ingestion Workflow",
          "tasks": [
            {
              "task_key": "orc_ingestion_task",
              "notebook_task": {
                "notebook_path": "/Workspace/Path/To/big_data_runner",
                "base_parameters": {
                  "config": "orc_ingestion.json",
                  "task_name": "orc_ingestion"
                }
              },
              "existing_cluster_id": "your-cluster-id"
            }
          ]
        }
        EOF
        databricks jobs create --json-file workflow_config.json
